#!python

"""
    Functions and algorithms to generate 
"""
import numpy as np 
import pandas as pd
from tqdm import tqdm
import allel
import sys
sys.path.append('src/')
from gen_ld_mat import *


# ------- Data Directories ------ # 
DATA_DIR  = '/project2/jnovembre/data/external_public/geo_LD/'
VCF_1KG_DIR = '/project2/jnovembre/data/external_public/1kg_phase3/haps/'


# ------ Global Variables ------ # 
TOT_N = 10

pops_total = ['GWD', 'CLM', 'BEB', 'PEL', 'LWK', 'MSL',
              'GBR', 'IBS', 'ASW', 'FIN', 'TSI', 'KHV',
              'CEU', 'YRI', 'CHB', 'STU', 'CHS', 'ESN',
              'ACB', 'GIH', 'PJL', 'MXL', 'ITU', 'CDX', 'JPT', 'PUR']

superpop_total = ['AFR','EUR','SAS','EAS','AMR']


# ------ Rules ------ # 
rule filt_biallelic_AC_1kg:
    """
       Filter to biallelic SNPs in the 1000 Genomes Dataset (with a particular
       allele count filter)
    """
    input:
      vcf =  VCF_1KG_DIR + 'ALL.chr{CHROM}.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz'
    output:
      vcf = DATA_DIR  + 'ALL.chr{CHROM}.phase3_shapeit2_mvncall_integrated.20130502.biallelic_snps.ac{AC,\d+}.genotypes.vcf.gz',
      vcf_idx = DATA_DIR  + 'ALL.chr{CHROM}.phase3_shapeit2_mvncall_integrated.20130502.biallelic_snps.ac{AC,\d+}.genotypes.vcf.gz.tbi' 
    shell:
      """
      bcftools view -v snps -m2 -M2 -c {wildcards.AC}:minor {input.vcf} | bgzip -@4 > {output.vcf}    
      tabix -f {output.vcf}
      """

rule gen_LD_single_pop_chrom:
    """
        Generate a K x P matrix of r^2 values for a single chromosome
    """
    input:
      vcf = rules.filt_biallelic_AC_1kg.output.vcf,
      popfile = VCF_1KG_DIR + 'integrated_call_samples_v3.20130502.ALL.panel'
    output:
      ld_mat = 'data/ld_mats/chr{CHROM}_ac{AC,\d+}_K{K,\d+}_pop{POP}.npz'
    run:
      pop_df = pd.read_csv(input.popfile, sep='\t')
      pop_vec = pop_df['pop'].values
      vcf_data = allel.read_vcf(input.vcf, fields=['variants/CHROM','variants/POS','calldata/GT'])
      chroms = vcf_data['variants/CHROM']
      positions = vcf_data['variants/POS']
      gt = vcf_data['calldata/GT']
      raw_gt = gt[:,:,0] + gt[:,:,1]
      raw_gt = raw_gt.astype(np.float32)
      raw_gt[raw_gt < 0] = np.nan
      ld_mat, alt_af = est_kxp_mat(raw_gt, pop_vec, wildcards.POP, int(wildcards.K))
      # Saving the compressed data
      np.savez_compressed(output.ld_mat, chrom=chroms, positions=positions, alt_af = alt_af, ld_mat = ld_mat)


rule gen_multipop_r2_matrices:
    input:
      expand('data/ld_mats/chr{CHROM}_ac{AC}_K{K}_pop{POP}.npz', AC=5, POP=['CEU'], K=[100], CHROM=22)

      
rule adaptive_k_single_pop_chrom:
    """
        Generate list containing r2 arrays for each SNP
        on a chromosome for a population using the adaptive algorithm
        NOTE : we should ideally have the input parameters for the algorithm in here as well ... 
    """
    input:
      vcf = rules.filt_biallelic_AC_1kg.output.vcf,
      popfile = VCF_1KG_DIR + 'integrated_call_samples_v3.20130502.ALL.panel'
    output:
      adaptive_ld_mat = 'data/ld_mats/chr{CHROM}_ac{AC,\d+}_pop{POP}.adaptive.npz'    
    run:
      # Reading in vcf
      callset = allel.read_vcf(input.vcf, fields=['calldata/GT','samples', 'variants/CHROM', 'variants/POS'])
      chroms = callset['variants/CHROM']
      positions = callset['variants/POS']
      # Generating genotype array
      gt = allel.GenotypeArray(callset['calldata/GT'])
      # Filtering out particular population
      panel = pd.read_csv(input.popfile, sep='\t', usecols=['sample', 'pop', 'super_pop'])
      pop_check = (panel['pop']==wildcards.POP)
      pop_indices = panel[pop_check].index.values
      pop_gt = gt.take(pop_indices, axis=1)
      # Creating summed genotype matrix
      gt_mat = pop_gt.to_n_alt()
      _, nsamp = gt_mat.shape
      # Calculate alt AF
      alt_af = np.sum(gt_mat, axis=1)/(2*nsamp)
      
      # Filter this to only the sites that are polymorphic
      idx_polymorphic = ((alt_af > 0) & (alt_af < 1))
      alt_af_filt = alt_af[idx_polymorphic]
      gt_mat_filt = gt_mat[idx_polymorphic,:]
      nsnps_filt, nsamp_filt = gt_mat_filt.shape
      
      # Empty list for arrays
      full_r2_list = []
      # Appending each array to list and saving
      for i in tqdm(range(nsnps_filt)):
        snp_r2_adaptive = adaptive_ld_mat_snp(gt_mat_filt, i)
        full_r2_list.append(snp_r2_adaptive)
      # Stack the array in a clever way
      # NOTE : we still might have to think of a better way to stack
      adaptive_ld_mat, idxs = stack_ragged(full_r2_list)
      np.savez_compressed(output.adaptive_ld_mat,
                          adaptive_ld_mat=adaptive_ld_mat, idx=idxs,
                          chroms=chroms[idx_polymorphic], positions=positions[idx_polymorphic], alt_af = alt_af_filt)
        
rule adaptive_ld_mats:
  input:
    expand('data/ld_mats/chr{CHROM}_ac{AC}_pop{POP}.adaptive.npz', CHROM=22, AC=5, POP=['CEU', 'CHB'])      
     
      
# ------- Split Versions ------- #       
rule split_kxp_per_chrom:
  """
    Split the data into TOT_N chunks for easier parallelization
  """
  input:
    vcf = rules.filt_biallelic_AC_1kg.output.vcf,
    popfile = VCF_1KG_DIR + 'integrated_call_samples_v3.20130502.ALL.panel'
  output:
    ld_mat_split = 'data/ld_mats/chr{CHROM,\d+}/filt_ac{AC,\d+}_K{K,\d+}_{poplist}_{POP}_split_{N,\d+}.npz'
  wildcard_constraints:
    poplist='(pop|superpop)'
  run:
      # Reading in the population file
      pop_df = pd.read_csv(input.popfile, sep='\t')
      if wildcards.poplist == 'superpop':
        pop_vec = pop_df['super_pop'].values
      else:
        pop_vec = pop_df['pop'].values
      
      vcf_data = allel.read_vcf(input.vcf, fields=['variants/CHROM','variants/POS','calldata/GT'])
      chroms = vcf_data['variants/CHROM']
      positions = vcf_data['variants/POS']
      gt = vcf_data['calldata/GT']
      n = int(wildcards.N)
      k = int(wildcards.K)
      # Do the filtering for shape here   
      P,N,_ = gt.shape
      split_idx = np.array_split(np.arange(P), TOT_N)
      split_idx_cur = split_idx[n]
      min_id, max_id = np.min(split_idx_cur), np.max(split_idx_cur)
      gt = gt[min_id:(max_id+k),:,:]
      # Sum up the genotpes & converting to floats
      raw_gt = gt[:,:,0] + gt[:,:,1]
      raw_gt = raw_gt.astype(np.float32)
      raw_gt[raw_gt < 0] = np.nan
      
      # Running the LD estimation from KxP for all snps in the segment
      ld_mat_split, alt_af = est_kxp_mat(raw_gt, pop_vec, wildcards.POP, k)
      if n != (TOT_N - 1):
        ld_mat_split = ld_mat_split[:,:-(k-1)]
        alt_af = alt_af[:-(k-1)]
        
      split_pos = positions[split_idx_cur]
      split_chroms = chroms[split_idx_cur]
      # Check that the positions match up with this here to avoid mismatching down the line ...       
      assert(split_pos.size == ld_mat_split.shape[1])
      assert(split_pos.size == alt_af.size)
      np.savez_compressed(output.ld_mat_split, 
                          chrom=split_chroms, 
                          positions=split_pos, 
                          alt_af=alt_af, 
                          ld_mat_split=ld_mat_split)



rule collapse_kxp_mats:
  """
    Collapsing the KxP matrices via np.hstack
  """
  input:
    input_split_data = expand('data/ld_mats/chr{{CHROM}}/filt_ac{{AC}}_K{{K}}_{{poplist}}_{{POP}}_split_{N}.npz', N = np.arange(TOT_N))
  output:
    ld_mat = 'data/ld_mats/chr{CHROM}/filt_ac{AC}_K{K}_{poplist}_{POP}_collected.npz'
  run:
    # Generate a set of collections 
    tot_kxp_mat = []
    tot_chroms = []
    tot_pos = []
    tot_alt_af = []
    for f in input.input_split_data:
      x = np.load(f, allow_pickle=True)
      # Appending all of the things!
      tot_kxp_mat.append(x['ld_mat_split'])
      tot_chroms.append(x['chrom'])
      tot_pos.append(x['positions'])
      tot_alt_af.append(x['alt_af'])
    # hstack everything now to make sure we have the correct appending 
    tot_kxp_true = np.hstack(tot_kxp_mat)
    tot_chrom_true = np.hstack(tot_chroms)
    tot_pos_true = np.hstack(tot_pos)
    tot_alt_af_true = np.hstack(tot_alt_af)
    
    # Make sure that the dimensions match up nicely ...
    assert(tot_alt_af_true.size == tot_pos_true.size)
    assert(tot_alt_af_true.size == tot_kxp_true.shape[1])
    # Save the compressed version of these plots here 
    np.savez_compressed(output.ld_mat, chrom=tot_chrom_true, positions=tot_pos_true, alt_af=tot_alt_af_true, ld_mat = tot_kxp_true)
    # Delete the initial files before we collapse them ... 
    for i in input.input_split_data:
      shell('rm -f %s' % i)

rule split_collapsed_ld_mats:
  input:
    expand('data/ld_mats/chr{CHROM}/filt_ac{AC}_K{K}_{poplist}_{POP}_collected.npz'
, CHROM=22, AC=5, K=[100], poplist='pop', POP='CEU'),
    expand('data/ld_mats/chr{CHROM}/filt_ac{AC}_K{K}_{poplist}_{POP}_collected.npz'
, CHROM=22, AC=5, K=[200], poplist='superpop', POP=superpop_total)
       


BINNING_DICT = {'linear' : np.linspace(0,1,255), 'log' : np.insert(np.logspace(-3,0,254), 0,0)}    
  
rule binning_ld_values_8bit:
    input:
      ld_non_collapsed = rules.collapse_kxp_mats.output.ld_mat
    output:
      ld_mat_binned = 'data/ld_mats/chr{CHROM, \d+}/filt_ac{AC,\d+}_K{K, \d+}_{poplist}_{POP}_binning{bins}.npz'
    wildcard_constraints:
      poplist='(pop|superpop)',
      bins='(log|linear)'
    run:
      x = np.load(input.ld_non_collapsed, allow_pickle=True)
      bins = BINNING_DICT[wildcards.bins]
      ld_mat = x['ld_mat']
      # NOTE : we might have to play this 
      ld_mat_binned = np.digitize(ld_mat, right=True)
      ld_mat_binned = ld_mat_binned.astype(np.uint8)
      np.savez_compressed(output.ld_mat_binned, chrom=x['chrom'], positions=x['positions'], alt_af=x['alt_af'], ld_mat = ld_mat_binned)

    
             
